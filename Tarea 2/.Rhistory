sdx=apply(x,2,sd)
#Para el set de testeo
xnew[,1]=(xnew[,1]-mx[1])/sdx[1]
xnew[,2]=(xnew[,2]-mx[2])/sdx[2]
#Para el set de entrenamiento
x.est=x
x.est[,1]=(x[,1]-mx[1])/sdx[1]
x.est[,2]=(x[,2]-mx[2])/sdx[2]
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## PARTE I: Kernel Ridge Regression                                 #
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~#
# Kernel function #
#~~~~~~~~~~~~~~~~~#
Kfun = function(x,y,l)
{
return(exp(-sum((x-y)^2)/l^2))
}
#Calculamos la Gramm matrix utilizando los datos en el set de entrenamiento
l=1
#x.rows=split(x.est,row(x.est))
#K=outer(x.rows,x.rows,Vectorize(function(x,y)Kfun(x,y,l)))
ntrain=dim(x.est)[1]
K=matrix(0,nrow=ntrain,ncol=ntrain)
for(i in 1:ntrain)
{
for(j in 1:ntrain)
{
K[i,j]=Kfun(x.est[i,],x.est[j,],l)
}
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Estimación de los coeficientes w asociados a la kernel regression #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#Resolvemos el problema de regresión
ntrain=dim(x)[1]
lambda=0.1 #parámetro de regularización
w.hat=solve(t(K)%*%K+lambda*diag(ntrain))%*%t(K)%*%y
#La variable w.hat guarda el valor estimador de los coeficientes.
w.hat
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Predicción de nuevos valores #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#En esta parte utilizamos w.hat para predecir la función que estimó nuestro modelo de regresión.
#Paso 1: Creamos una grilla 2-dimensional (ya que nuestras covariables viven en R^2) en donde predecimos nuevos valores:
glim=5
s=seq(0,glim,0.05)
s2=as.matrix(expand.grid(s,s))
#Paso 2: Para predecir nuevos valores debermos usar la misma estandarización que utilizamos en el set de entrenamiento:
s2.est=s2
s2.est[,1]=(s2[,1]-mx[1])/sdx[1]
s2.est[,2]=(s2[,2]-mx[2])/sdx[2]
dim(s2.est)
npred=dim(s2.est)[1] #npred guarda el número de nuevas observaciones
#Paso 3: Calculamos la matriz de diseño predictiva
Kppred=matrix(0,nrow=npred,ncol=ntrain)
dim(Kppred)
for(i in 1:ntrain)
{
Kppred[,i]=exp(-rowSums((s2.est-matrix(x.est[i,],nrow=npred,ncol = 2,byrow=TRUE))^2)/l^2)
}
#Calculamos la estimación
FitKernel=Kppred%*%w.hat
#Graficamos nuestra estimación
# plot3d(x[,1],x[,2],y,col=2)
# points3d(s2[,1],s2[,2],FitKernel, col = 3)
#
# #Ahora añadimos la verdadera función para evaluar la calidad de nuestra estimación
# yfun=function(x,z)5*sin(2*x)*cos(z)+sqrt(z)-x^(1/3)-0.1
# mfrow3d(1, 2, sharedMouse = TRUE)
# plot3d(x[,1],x[,2],y,col=2)
# points3d(s2[,1],s2[,2],FitKernel, col = 3)
# plot3d(yfun,xlim = c(0,glim), ylim = c(0,glim))
#~~~~~~~~~~~~~~~~~~#
# Error predictivo #
#~~~~~~~~~~~~~~~~~~#
# plot3d(x[,1],x[,2],y,col=2)
# points3d(s2[,1],s2[,2],FitKernel, col = 3)
# spheres3d(test$x[,1],test$x[,2],test$y, col = "blue", r=0.1)
#Procedemos a calcular el error cuadratico medio predictivo
ntest=dim(xnew)[1]
Kppred2=matrix(0,nrow=ntest,ncol=ntrain)
for(i in 1:ntrain)
{
Kppred2[,i]=exp(-rowSums((xnew-matrix(x.est[i,],nrow=ntest,ncol=2,byrow=TRUE))^2)/l^2)
}
ypred=Kppred2%*%w.hat
1/ntest*sum((ynew-ypred)^2)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
?rowSums
rsq = 1 - (rowSums(y_test - y_hat)/rowSums(y_test - y_pred_test))
rsq
rsq = 1 - ((y_test - y_hat)/(y_test - y_pred_test))
rsq
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
### Script P2 Tarea 2 Kernels
rm(list = ls())
set.seed(23)
path = '~/OneDrive - Universidad Adolfo Ibanez/Codes/Universidad/Kernels/Repo/Tarea 2/sim.txt'
data = read.delim(path, header = TRUE, sep = ",", dec = ".")
data = data[sample(nrow(data)),]
# Estableciendo 10-fold CV (https://gist.github.com/duttashi/a51c71acb7388c535e30b57854598e77)
folds <- cut(seq(1, nrow(data)), breaks = 10, labels=FALSE)
#Perform 10 fold cross validation
for(i in 1:10){
#Segement your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
testa <- data[testIndexes, ]
traina <- data[-testIndexes, ]
#Use the test and train data partitions however you desire...
}
folds
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
###################################################################
## Clase: Kernel regression                                      ##
##                                                               ##
## Temas: Kernel regression                                      ##
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ##
## Curso : Métodos basados en kernel para aprendizaje automático ##
## Profesor: Tamara Fernandez                                    ##
## Fecha :                                                       ##
###################################################################
rm(list = ls())
#library("rgl")
#~~~~~~~~~~~~~~~~~#
# Datos Simulados #
#~~~~~~~~~~~~~~~~~#
# En este script veremos un ejemplo de datos donde la covariable de interés pertenece a R^2.
set.seed(201207)
n = 200
pointX=cbind(runif(n, 0,5),runif(n, 0,5))
pointY=5*sin(2*pointX[,1])*cos(pointX[,2])+sqrt(pointX[,2])-pointX[,1]^(1/3)-0.1+rnorm(n,0,1/5)
index=sample(1:n,floor(0.75*n))
train=list(x=pointX[index,], y=pointY[index])
test=list(x=pointX[-index,], y=pointY[-index])
#~~~~~~~~~~~~~~~#
# Train y Test  #
#~~~~~~~~~~~~~~~#
#Separamos nuestros datos en un set de entrenamiento y un set de testeo:
#Para el set de entrenamiento:
x=train[["x"]]
y=train[["y"]]
#Para el set de testeo:
xnew=test[["x"]]
ynew=test[["y"]]
#Procedemos a graficar los datos en el set de entrenamiento. ¿Puede obserbar el patrón?
#plot3d(x[,1],x[,2],y,col=2)
#~~~~~~~~~~~~~~~~~~#
# Estandarización  #
#~~~~~~~~~~~~~~~~~~#
#En este módulo estandarizamos las covariables (en R^2) pertenecientes al set de entrenamiento y al set de testeo.
mx=apply(x,2,mean)
sdx=apply(x,2,sd)
#Para el set de testeo
xnew[,1]=(xnew[,1]-mx[1])/sdx[1]
xnew[,2]=(xnew[,2]-mx[2])/sdx[2]
#Para el set de entrenamiento
x.est=x
x.est[,1]=(x[,1]-mx[1])/sdx[1]
x.est[,2]=(x[,2]-mx[2])/sdx[2]
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## PARTE I: Kernel Ridge Regression                                 #
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~#
# Kernel function #
#~~~~~~~~~~~~~~~~~#
Kfun = function(x,y,l)
{
return(exp(-sum((x-y)^2)/l^2))
}
#Calculamos la Gramm matrix utilizando los datos en el set de entrenamiento
l=1
#x.rows=split(x.est,row(x.est))
#K=outer(x.rows,x.rows,Vectorize(function(x,y)Kfun(x,y,l)))
ntrain=dim(x.est)[1]
K=matrix(0,nrow=ntrain,ncol=ntrain)
for(i in 1:ntrain)
{
for(j in 1:ntrain)
{
K[i,j]=Kfun(x.est[i,],x.est[j,],l)
}
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Estimación de los coeficientes w asociados a la kernel regression #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#Resolvemos el problema de regresión
ntrain=dim(x)[1]
lambda=0.1 #parámetro de regularización
w.hat=solve(t(K)%*%K+lambda*diag(ntrain))%*%t(K)%*%y
#La variable w.hat guarda el valor estimador de los coeficientes.
w.hat
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Predicción de nuevos valores #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#En esta parte utilizamos w.hat para predecir la función que estimó nuestro modelo de regresión.
#Paso 1: Creamos una grilla 2-dimensional (ya que nuestras covariables viven en R^2) en donde predecimos nuevos valores:
glim=5
s=seq(0,glim,0.05)
s2=as.matrix(expand.grid(s,s))
#Paso 2: Para predecir nuevos valores debermos usar la misma estandarización que utilizamos en el set de entrenamiento:
s2.est=s2
s2.est[,1]=(s2[,1]-mx[1])/sdx[1]
s2.est[,2]=(s2[,2]-mx[2])/sdx[2]
dim(s2.est)
npred=dim(s2.est)[1] #npred guarda el número de nuevas observaciones
#Paso 3: Calculamos la matriz de diseño predictiva
Kppred=matrix(0,nrow=npred,ncol=ntrain)
dim(Kppred)
for(i in 1:ntrain)
{
Kppred[,i]=exp(-rowSums((s2.est-matrix(x.est[i,],nrow=npred,ncol = 2,byrow=TRUE))^2)/l^2)
}
#Calculamos la estimación
FitKernel=Kppred%*%w.hat
#Graficamos nuestra estimación
# plot3d(x[,1],x[,2],y,col=2)
# points3d(s2[,1],s2[,2],FitKernel, col = 3)
#
# #Ahora añadimos la verdadera función para evaluar la calidad de nuestra estimación
# yfun=function(x,z)5*sin(2*x)*cos(z)+sqrt(z)-x^(1/3)-0.1
# mfrow3d(1, 2, sharedMouse = TRUE)
# plot3d(x[,1],x[,2],y,col=2)
# points3d(s2[,1],s2[,2],FitKernel, col = 3)
# plot3d(yfun,xlim = c(0,glim), ylim = c(0,glim))
#~~~~~~~~~~~~~~~~~~#
# Error predictivo #
#~~~~~~~~~~~~~~~~~~#
# plot3d(x[,1],x[,2],y,col=2)
# points3d(s2[,1],s2[,2],FitKernel, col = 3)
# spheres3d(test$x[,1],test$x[,2],test$y, col = "blue", r=0.1)
#Procedemos a calcular el error cuadratico medio predictivo
ntest=dim(xnew)[1]
Kppred2=matrix(0,nrow=ntest,ncol=ntrain)
for(i in 1:ntrain)
{
Kppred2[,i]=exp(-rowSums((xnew-matrix(x.est[i,],nrow=ntest,ncol=2,byrow=TRUE))^2)/l^2)
}
ypred=Kppred2%*%w.hat
1/ntest*sum((ynew-ypred)^2)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Elección de los parámetros l y lambda  #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
l.grid=seq(0.1,2,0.1)
lambda.grid=seq(0.1,2,0.1)
#En el vector aux escribimos todas las combinaciones de valores de l.grid x lambda.grid
aux=expand.grid(l.grid,lambda.grid)
#Procedemos a calcular el error cuadratico medio para todos los valores de la grilla aux. Guardamos estos valores en out.
out=rep(0,dim(aux)[1])
for(i in 1:dim(aux)[1])
{
print(paste("Iter:",i))
x.rows=split(x.est,row(x.est))
K=outer(x.rows,x.rows,Vectorize(function(x,y)Kfun(x,y,aux[i,1])))
w.hat=solve(K+aux[i,2]*diag(ntrain))%*%y
Ktest=matrix(0,nrow=ntest,ncol=ntrain)
for(j in 1:ntrain)
{
Ktest[,j]=exp(-rowSums((xnew-matrix(x.est[j,],nrow=ntest,ncol=2,byrow=TRUE))^2)/aux[i,1]^2)
}
ypred=Ktest%*%w.hat
out[i]=1/ntest*sum((ynew-ypred)^2)
}
#plot3d(aux[,1],aux[,2],out,col=2)
#Encontramos los parámetros óptimos:
indice=which(out==min(out))
aux[indice,]
View(test)
View(out)
dim(aux[1])
x.rows
dim(aux[1])
dim(aux)[1]
val = rep(0, dim_grid)
aux[indice,]
aux[indice,][0]
aux[indice,][1]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
llgrid[index,][1]
l_vec
llgrid[index,][1][1]
llgrid[index,][1][2]
llgrid[index,]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
l_vec
llgrid[index,]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
llgrid[index,]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
llgrid[index,]
llgrid[index,][1,1]
ecm_save[1]
ecm_save[25]
ecm_save[3]
ecm_save[8]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
rsq_vec2
l_vec
l_vec[1]
l_vec[2]
l_vec[2]
llgrid[index,]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
llgrid[index,]
llgrid
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
llgrid[index,]
llgrid[index,][1,1]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
rsq_vec2
l_vec[1]
l_vec[1][1]
l_vec[2]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
l_vec[1]
l_vec[i]
l_vec[i] + lam_vec[1]
typeof(l_vec)
typeof(l_vec[1])
typeof(l_vec[1][1])
typeof(l_vec[1][2])
l_vec[1][2]
unlist(l_vec)
typeof(unlist(l_vec[1]))
unlist(l_vec) + unlist(lam_vec)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
View(l_vec)
### Script P2 Tarea 2 Kernels
rm(list = ls())
set.seed(23)
path = '~/OneDrive - Universidad Adolfo Ibanez/Codes/Universidad/Kernels/Repo/Tarea 2/sim.txt'
data = read.delim(path, header = TRUE, sep = ",", dec = ".")
data = data[sample(nrow(data)),]
### Pregunta 1
# Estableciendo 10-fold CV
folds <- cut(seq(1, nrow(data)), breaks = 10, labels=FALSE)
ecm_vec = c()
rsq_vec = c()
for(i in 1:10){
print(paste("Fold: ",i))
testIndexes <- which(folds==i,arr.ind=TRUE)
test <- data[testIndexes, ]
train <- data[-testIndexes, ]
x_train = train[1:2]
y_train = unlist(train[3])
x_test = test[1:2]
y_test = unlist(test[3])
# Estandarizando
mux = apply(x_train,2,mean)
sdx = apply(x_train,2,sd)
xtrain_est = x_train
xtrain_est[,1] = (x_train[,1] - mux[1])/sdx[1]
xtrain_est[,2] = (x_train[,2] - mux[2])/sdx[2]
xtest_est = x_test
xtest_est[,1] = (x_test[,1] - mux[1])/sdx[1]
xtest_est[,2] = (x_test[,2] - mux[2])/sdx[2]
# Función de Kernel
Kfun = function(xi,xj,l2=0.25)
{
exp(-sum((xi-xj)^2)/l2)
}
# Matriz de Gram
ntrain = dim(xtrain_est)[1]
K = matrix(0, nrow = ntrain, ncol = ntrain)
for(i in 1:ntrain)
{
for(j in 1:ntrain)
{
K[i,j] = Kfun(xtrain_est[i,], xtrain_est[j,])
}
}
# Calculando W hat e Y hat
lambda = 1
w_hat = solve(t(K)%*%K+lambda*diag(ntrain))%*%t(K)%*%y_train
y_hat = K%*%w_hat
# Revisando valores mínimos y máximos para armar parrilla
min(data[1]) # 0.06019702
max(data[1]) # 4.976222
min(data[2]) # 0.01729765
max(data[2]) # 4.985938
s = seq(0, 5, 0.05)
sgrid = as.matrix(expand.grid(s,s))
sgrid_est = sgrid
sgrid_est[,1]=(sgrid_est[,1] - mux[1])/sdx[1]
sgrid_est[,2]=(sgrid_est[,2] - mux[2])/sdx[2]
## Armando predicciones con el dataset de entrenamiento
#xtrain_est = unlist(xtrain_est)
npred = dim(sgrid_est)[1]
K_pred_train = matrix(0, nrow = npred, ncol = ntrain)
for(i in 1:ntrain)
{
K_pred_train[,i]=exp(-rowSums((sgrid_est - matrix(unlist(xtrain_est[i,]), nrow = npred, ncol = 2, byrow = TRUE))^2)/0.25)
}
y_pred_train = K_pred_train%*%w_hat
## Armando predicciones con el dataset de testeo
ntest = dim(xtest_est)[1]
K_pred_test=matrix(0, nrow = ntest, ncol = ntrain)
for(i in 1:ntrain)
{
K_pred_test[,i]=exp(-rowSums((xtest_est - matrix(unlist(xtrain_est[i,]), nrow = ntest, ncol = 2, byrow = TRUE))^2)/0.25)
}
y_pred_test = K_pred_test%*%w_hat
## Error cuadrático medio y R cuadrado
ecm = 1/ntest*sum((y_test - y_pred_test)^2)
ecm_vec = append(ecm_vec,ecm)
rsq = cor(y_test, y_pred_test)^2
rsq_vec = append(rsq_vec, rsq)
}
ecm_vec
rsq_vec
boxplot(ecm_vec)
boxplot(rsq_vec)
?boxplot
### Script P2 Tarea 2 Kernels
rm(list = ls())
set.seed(23)
path = '~/OneDrive - Universidad Adolfo Ibanez/Codes/Universidad/Kernels/Repo/Tarea 2/sim.txt'
data = read.delim(path, header = TRUE, sep = ",", dec = ".")
data = data[sample(nrow(data)),]
### Pregunta 1
# Estableciendo 10-fold CV
folds <- cut(seq(1, nrow(data)), breaks = 10, labels=FALSE)
ecm_vec = c()
rsq_vec = c()
for(i in 1:10){
print(paste("Fold: ",i))
testIndexes <- which(folds==i,arr.ind=TRUE)
test <- data[testIndexes, ]
train <- data[-testIndexes, ]
x_train = train[1:2]
y_train = unlist(train[3])
x_test = test[1:2]
y_test = unlist(test[3])
# Estandarizando
mux = apply(x_train,2,mean)
sdx = apply(x_train,2,sd)
xtrain_est = x_train
xtrain_est[,1] = (x_train[,1] - mux[1])/sdx[1]
xtrain_est[,2] = (x_train[,2] - mux[2])/sdx[2]
xtest_est = x_test
xtest_est[,1] = (x_test[,1] - mux[1])/sdx[1]
xtest_est[,2] = (x_test[,2] - mux[2])/sdx[2]
# Función de Kernel
Kfun = function(xi,xj,l2=0.25)
{
exp(-sum((xi-xj)^2)/l2)
}
# Matriz de Gram
ntrain = dim(xtrain_est)[1]
K = matrix(0, nrow = ntrain, ncol = ntrain)
for(i in 1:ntrain)
{
for(j in 1:ntrain)
{
K[i,j] = Kfun(xtrain_est[i,], xtrain_est[j,])
}
}
# Calculando W hat e Y hat
lambda = 1
w_hat = solve(t(K)%*%K+lambda*diag(ntrain))%*%t(K)%*%y_train
y_hat = K%*%w_hat
# Revisando valores mínimos y máximos para armar parrilla
min(data[1]) # 0.06019702
max(data[1]) # 4.976222
min(data[2]) # 0.01729765
max(data[2]) # 4.985938
s = seq(0, 5, 0.05)
sgrid = as.matrix(expand.grid(s,s))
sgrid_est = sgrid
sgrid_est[,1]=(sgrid_est[,1] - mux[1])/sdx[1]
sgrid_est[,2]=(sgrid_est[,2] - mux[2])/sdx[2]
## Armando predicciones con el dataset de entrenamiento
#xtrain_est = unlist(xtrain_est)
npred = dim(sgrid_est)[1]
K_pred_train = matrix(0, nrow = npred, ncol = ntrain)
for(i in 1:ntrain)
{
K_pred_train[,i]=exp(-rowSums((sgrid_est - matrix(unlist(xtrain_est[i,]), nrow = npred, ncol = 2, byrow = TRUE))^2)/0.25)
}
y_pred_train = K_pred_train%*%w_hat
## Armando predicciones con el dataset de testeo
ntest = dim(xtest_est)[1]
K_pred_test=matrix(0, nrow = ntest, ncol = ntrain)
for(i in 1:ntrain)
{
K_pred_test[,i]=exp(-rowSums((xtest_est - matrix(unlist(xtrain_est[i,]), nrow = ntest, ncol = 2, byrow = TRUE))^2)/0.25)
}
y_pred_test = K_pred_test%*%w_hat
## Error cuadrático medio y R cuadrado
ecm = 1/ntest*sum((y_test - y_pred_test)^2)
ecm_vec = append(ecm_vec,ecm)
rsq = cor(y_test, y_pred_test)^2
rsq_vec = append(rsq_vec, rsq)
}
ecm_vec
rsq_vec
boxplot(ecm_vec, main = 'Error Cuadrático Medio')
boxplot(rsq_vec, main = 'R Squared')
e2 = [0.12811460, 0.08119819, 0.14190369, 0.14383414, 0.06243915, 0.14208905, 0.09340598, 0.21809802, 0.11342908, 0.11131569]
e2 = c(0.12811460, 0.08119819, 0.14190369, 0.14383414, 0.06243915, 0.14208905, 0.09340598, 0.21809802, 0.11342908, 0.11131569)
r2 = c(0.9693284, 0.9812473, 0.9457643, 0.9679858, 0.9832975, 0.9789571, 0.9888192, 0.9504604, 0.9702719, 0.9581499)
boxplot(e2, main = 'Error Cuadrático Medio')
boxplot(r2, main = 'R Squared')
