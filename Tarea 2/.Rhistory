}
dim(xtrain_est)
View(y_hat)
xtrain_est
xtrain_est[2151]
xtrain_est[21]
xtrain_est[2150]
sapply(xtrain_est, typeof)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
sapply(xtrain_est, typeof)
xtrain_est[1]
xtrain_est[1,1]
xtrain_est[1,2]
xtrain_est[,2]
xtrain_est[1,]
typeof(sgrid)
typeof(xtrain_est[1,])
unlist(xtrain_est[1,])
typeof(unlist(xtrain_est[1,]))
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
ntrain
unlist(xtrain_est[151,])
npred
###################################################################
## Clase: Kernel regression                                      ##
##                                                               ##
## Temas: Kernel regression                                      ##
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ##
## Curso : Métodos basados en kernel para aprendizaje automático ##
## Profesor: Tamara Fernandez                                    ##
## Fecha :                                                       ##
###################################################################
rm(list = ls())
#library("rgl")
#~~~~~~~~~~~~~~~~~#
# Datos Simulados #
#~~~~~~~~~~~~~~~~~#
# En este script veremos un ejemplo de datos donde la covariable de interés pertenece a R^2.
set.seed(201207)
n = 200
pointX=cbind(runif(n, 0,5),runif(n, 0,5))
pointY=5*sin(2*pointX[,1])*cos(pointX[,2])+sqrt(pointX[,2])-pointX[,1]^(1/3)-0.1+rnorm(n,0,1/5)
index=sample(1:n,floor(0.75*n))
train=list(x=pointX[index,], y=pointY[index])
test=list(x=pointX[-index,], y=pointY[-index])
#~~~~~~~~~~~~~~~#
# Train y Test  #
#~~~~~~~~~~~~~~~#
#Separamos nuestros datos en un set de entrenamiento y un set de testeo:
#Para el set de entrenamiento:
x=train[["x"]]
y=train[["y"]]
#Para el set de testeo:
xnew=test[["x"]]
ynew=test[["y"]]
#Procedemos a graficar los datos en el set de entrenamiento. ¿Puede obserbar el patrón?
#plot3d(x[,1],x[,2],y,col=2)
#~~~~~~~~~~~~~~~~~~#
# Estandarización  #
#~~~~~~~~~~~~~~~~~~#
#En este módulo estandarizamos las covariables (en R^2) pertenecientes al set de entrenamiento y al set de testeo.
mx=apply(x,2,mean)
sdx=apply(x,2,sd)
#Para el set de testeo
xnew[,1]=(xnew[,1]-mx[1])/sdx[1]
xnew[,2]=(xnew[,2]-mx[2])/sdx[2]
#Para el set de entrenamiento
x.est=x
x.est[,1]=(x[,1]-mx[1])/sdx[1]
x.est[,2]=(x[,2]-mx[2])/sdx[2]
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## PARTE I: Kernel Ridge Regression                                 #
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~#
# Kernel function #
#~~~~~~~~~~~~~~~~~#
Kfun = function(x,y,l)
{
return(exp(-sum((x-y)^2)/l^2))
}
#Calculamos la Gramm matrix utilizando los datos en el set de entrenamiento
l=1
#x.rows=split(x.est,row(x.est))
#K=outer(x.rows,x.rows,Vectorize(function(x,y)Kfun(x,y,l)))
ntrain=dim(x.est)[1]
K=matrix(0,nrow=ntrain,ncol=ntrain)
for(i in 1:ntrain)
{
for(j in 1:ntrain)
{
K[i,j]=Kfun(x.est[i,],x.est[j,],l)
}
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Estimación de los coeficientes w asociados a la kernel regression #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#Resolvemos el problema de regresión
ntrain=dim(x)[1]
lambda=0.1 #parámetro de regularización
w.hat=solve(t(K)%*%K+lambda*diag(ntrain))%*%t(K)%*%y
#La variable w.hat guarda el valor estimador de los coeficientes.
w.hat
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Predicción de nuevos valores #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#En esta parte utilizamos w.hat para predecir la función que estimó nuestro modelo de regresión.
#Paso 1: Creamos una grilla 2-dimensional (ya que nuestras covariables viven en R^2) en donde predecimos nuevos valores:
glim=5
s=seq(0,glim,0.05)
s2=as.matrix(expand.grid(s,s))
#Paso 2: Para predecir nuevos valores debermos usar la misma estandarización que utilizamos en el set de entrenamiento:
s2.est=s2
s2.est[,1]=(s2[,1]-mx[1])/sdx[1]
s2.est[,2]=(s2[,2]-mx[2])/sdx[2]
dim(s2.est)
npred=dim(s2.est)[1] #npred guarda el número de nuevas observaciones
#Paso 3: Calculamos la matriz de diseño predictiva
Kppred=matrix(0,nrow=npred,ncol=ntrain)
dim(Kppred)
for(i in 1:ntrain)
{
Kppred[,i]=exp(-rowSums((s2.est-matrix(x.est[i,],nrow=npred,ncol = 2,byrow=TRUE))^2)/l^2)
}
#Calculamos la estimación
FitKernel=Kppred%*%w.hat
npred
ntrain
Kppred
typeof(Kppred)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
ntest = dim(xtest_est)[1]
K_pred_test = matrix(0, nrow = ntest, ncol = ntrain)
for(i in 1:ntrain)
{
K_pred_train[,i] = exp(-rowSums((xtest_est - matrix(unlist(xtrain_est[i,]), nrow = ntest, ncol = 2, byrow=TRUE))^2)/0.25)
}
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/KernelRegression.r", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/KernelRegression.r", echo=TRUE)
###################################################################
## Clase: Kernel regression                                      ##
##                                                               ##
## Temas: Kernel regression                                      ##
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ##
## Curso : Métodos basados en kernel para aprendizaje automático ##
## Profesor: Tamara Fernandez                                    ##
## Fecha :                                                       ##
###################################################################
rm(list = ls())
#library("rgl")
#~~~~~~~~~~~~~~~~~#
# Datos Simulados #
#~~~~~~~~~~~~~~~~~#
# En este script veremos un ejemplo de datos donde la covariable de interés pertenece a R^2.
set.seed(201207)
n = 200
pointX=cbind(runif(n, 0,5),runif(n, 0,5))
pointY=5*sin(2*pointX[,1])*cos(pointX[,2])+sqrt(pointX[,2])-pointX[,1]^(1/3)-0.1+rnorm(n,0,1/5)
index=sample(1:n,floor(0.75*n))
train=list(x=pointX[index,], y=pointY[index])
test=list(x=pointX[-index,], y=pointY[-index])
#~~~~~~~~~~~~~~~#
# Train y Test  #
#~~~~~~~~~~~~~~~#
#Separamos nuestros datos en un set de entrenamiento y un set de testeo:
#Para el set de entrenamiento:
x=train[["x"]]
y=train[["y"]]
#Para el set de testeo:
xnew=test[["x"]]
ynew=test[["y"]]
#Procedemos a graficar los datos en el set de entrenamiento. ¿Puede obserbar el patrón?
#plot3d(x[,1],x[,2],y,col=2)
#~~~~~~~~~~~~~~~~~~#
# Estandarización  #
#~~~~~~~~~~~~~~~~~~#
#En este módulo estandarizamos las covariables (en R^2) pertenecientes al set de entrenamiento y al set de testeo.
mx=apply(x,2,mean)
sdx=apply(x,2,sd)
#Para el set de testeo
xnew[,1]=(xnew[,1]-mx[1])/sdx[1]
xnew[,2]=(xnew[,2]-mx[2])/sdx[2]
#Para el set de entrenamiento
x.est=x
x.est[,1]=(x[,1]-mx[1])/sdx[1]
x.est[,2]=(x[,2]-mx[2])/sdx[2]
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## PARTE I: Kernel Ridge Regression                                 #
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~#
# Kernel function #
#~~~~~~~~~~~~~~~~~#
Kfun = function(x,y,l)
{
return(exp(-sum((x-y)^2)/l^2))
}
#Calculamos la Gramm matrix utilizando los datos en el set de entrenamiento
l=1
#x.rows=split(x.est,row(x.est))
#K=outer(x.rows,x.rows,Vectorize(function(x,y)Kfun(x,y,l)))
ntrain=dim(x.est)[1]
K=matrix(0,nrow=ntrain,ncol=ntrain)
for(i in 1:ntrain)
{
for(j in 1:ntrain)
{
K[i,j]=Kfun(x.est[i,],x.est[j,],l)
}
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Estimación de los coeficientes w asociados a la kernel regression #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#Resolvemos el problema de regresión
ntrain=dim(x)[1]
lambda=0.1 #parámetro de regularización
w.hat=solve(t(K)%*%K+lambda*diag(ntrain))%*%t(K)%*%y
#La variable w.hat guarda el valor estimador de los coeficientes.
w.hat
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Predicción de nuevos valores #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#En esta parte utilizamos w.hat para predecir la función que estimó nuestro modelo de regresión.
#Paso 1: Creamos una grilla 2-dimensional (ya que nuestras covariables viven en R^2) en donde predecimos nuevos valores:
glim=5
s=seq(0,glim,0.05)
s2=as.matrix(expand.grid(s,s))
#Paso 2: Para predecir nuevos valores debermos usar la misma estandarización que utilizamos en el set de entrenamiento:
s2.est=s2
s2.est[,1]=(s2[,1]-mx[1])/sdx[1]
s2.est[,2]=(s2[,2]-mx[2])/sdx[2]
dim(s2.est)
npred=dim(s2.est)[1] #npred guarda el número de nuevas observaciones
#Paso 3: Calculamos la matriz de diseño predictiva
Kppred=matrix(0,nrow=npred,ncol=ntrain)
dim(Kppred)
for(i in 1:ntrain)
{
Kppred[,i]=exp(-rowSums((s2.est-matrix(x.est[i,],nrow=npred,ncol = 2,byrow=TRUE))^2)/l^2)
}
#Calculamos la estimación
FitKernel=Kppred%*%w.hat
#Graficamos nuestra estimación
# plot3d(x[,1],x[,2],y,col=2)
# points3d(s2[,1],s2[,2],FitKernel, col = 3)
#
# #Ahora añadimos la verdadera función para evaluar la calidad de nuestra estimación
# yfun=function(x,z)5*sin(2*x)*cos(z)+sqrt(z)-x^(1/3)-0.1
# mfrow3d(1, 2, sharedMouse = TRUE)
# plot3d(x[,1],x[,2],y,col=2)
# points3d(s2[,1],s2[,2],FitKernel, col = 3)
# plot3d(yfun,xlim = c(0,glim), ylim = c(0,glim))
#~~~~~~~~~~~~~~~~~~#
# Error predictivo #
#~~~~~~~~~~~~~~~~~~#
# plot3d(x[,1],x[,2],y,col=2)
# points3d(s2[,1],s2[,2],FitKernel, col = 3)
# spheres3d(test$x[,1],test$x[,2],test$y, col = "blue", r=0.1)
#Procedemos a calcular el error cuadratico medio predictivo
ntest=dim(xnew)[1]
Kppred2=matrix(0,nrow=ntest,ncol=ntrain)
for(i in 1:ntrain)
{
Kppred2[,i]=exp(-rowSums((xnew-matrix(x.est[i,],nrow=ntest,ncol=2,byrow=TRUE))^2)/l^2)
}
ypred=Kppred2%*%w.hat
1/ntest*sum((ynew-ypred)^2)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
?rowSums
rsq = 1 - (rowSums(y_test - y_hat)/rowSums(y_test - y_pred_test))
rsq
rsq = 1 - ((y_test - y_hat)/(y_test - y_pred_test))
rsq
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
### Script P2 Tarea 2 Kernels
rm(list = ls())
set.seed(23)
path = '~/OneDrive - Universidad Adolfo Ibanez/Codes/Universidad/Kernels/Repo/Tarea 2/sim.txt'
data = read.delim(path, header = TRUE, sep = ",", dec = ".")
data = data[sample(nrow(data)),]
# Estableciendo 10-fold CV (https://gist.github.com/duttashi/a51c71acb7388c535e30b57854598e77)
folds <- cut(seq(1, nrow(data)), breaks = 10, labels=FALSE)
#Perform 10 fold cross validation
for(i in 1:10){
#Segement your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
testa <- data[testIndexes, ]
traina <- data[-testIndexes, ]
#Use the test and train data partitions however you desire...
}
folds
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
###################################################################
## Clase: Kernel regression                                      ##
##                                                               ##
## Temas: Kernel regression                                      ##
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ##
## Curso : Métodos basados en kernel para aprendizaje automático ##
## Profesor: Tamara Fernandez                                    ##
## Fecha :                                                       ##
###################################################################
rm(list = ls())
#library("rgl")
#~~~~~~~~~~~~~~~~~#
# Datos Simulados #
#~~~~~~~~~~~~~~~~~#
# En este script veremos un ejemplo de datos donde la covariable de interés pertenece a R^2.
set.seed(201207)
n = 200
pointX=cbind(runif(n, 0,5),runif(n, 0,5))
pointY=5*sin(2*pointX[,1])*cos(pointX[,2])+sqrt(pointX[,2])-pointX[,1]^(1/3)-0.1+rnorm(n,0,1/5)
index=sample(1:n,floor(0.75*n))
train=list(x=pointX[index,], y=pointY[index])
test=list(x=pointX[-index,], y=pointY[-index])
#~~~~~~~~~~~~~~~#
# Train y Test  #
#~~~~~~~~~~~~~~~#
#Separamos nuestros datos en un set de entrenamiento y un set de testeo:
#Para el set de entrenamiento:
x=train[["x"]]
y=train[["y"]]
#Para el set de testeo:
xnew=test[["x"]]
ynew=test[["y"]]
#Procedemos a graficar los datos en el set de entrenamiento. ¿Puede obserbar el patrón?
#plot3d(x[,1],x[,2],y,col=2)
#~~~~~~~~~~~~~~~~~~#
# Estandarización  #
#~~~~~~~~~~~~~~~~~~#
#En este módulo estandarizamos las covariables (en R^2) pertenecientes al set de entrenamiento y al set de testeo.
mx=apply(x,2,mean)
sdx=apply(x,2,sd)
#Para el set de testeo
xnew[,1]=(xnew[,1]-mx[1])/sdx[1]
xnew[,2]=(xnew[,2]-mx[2])/sdx[2]
#Para el set de entrenamiento
x.est=x
x.est[,1]=(x[,1]-mx[1])/sdx[1]
x.est[,2]=(x[,2]-mx[2])/sdx[2]
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## PARTE I: Kernel Ridge Regression                                 #
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~#
# Kernel function #
#~~~~~~~~~~~~~~~~~#
Kfun = function(x,y,l)
{
return(exp(-sum((x-y)^2)/l^2))
}
#Calculamos la Gramm matrix utilizando los datos en el set de entrenamiento
l=1
#x.rows=split(x.est,row(x.est))
#K=outer(x.rows,x.rows,Vectorize(function(x,y)Kfun(x,y,l)))
ntrain=dim(x.est)[1]
K=matrix(0,nrow=ntrain,ncol=ntrain)
for(i in 1:ntrain)
{
for(j in 1:ntrain)
{
K[i,j]=Kfun(x.est[i,],x.est[j,],l)
}
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Estimación de los coeficientes w asociados a la kernel regression #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#Resolvemos el problema de regresión
ntrain=dim(x)[1]
lambda=0.1 #parámetro de regularización
w.hat=solve(t(K)%*%K+lambda*diag(ntrain))%*%t(K)%*%y
#La variable w.hat guarda el valor estimador de los coeficientes.
w.hat
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Predicción de nuevos valores #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#En esta parte utilizamos w.hat para predecir la función que estimó nuestro modelo de regresión.
#Paso 1: Creamos una grilla 2-dimensional (ya que nuestras covariables viven en R^2) en donde predecimos nuevos valores:
glim=5
s=seq(0,glim,0.05)
s2=as.matrix(expand.grid(s,s))
#Paso 2: Para predecir nuevos valores debermos usar la misma estandarización que utilizamos en el set de entrenamiento:
s2.est=s2
s2.est[,1]=(s2[,1]-mx[1])/sdx[1]
s2.est[,2]=(s2[,2]-mx[2])/sdx[2]
dim(s2.est)
npred=dim(s2.est)[1] #npred guarda el número de nuevas observaciones
#Paso 3: Calculamos la matriz de diseño predictiva
Kppred=matrix(0,nrow=npred,ncol=ntrain)
dim(Kppred)
for(i in 1:ntrain)
{
Kppred[,i]=exp(-rowSums((s2.est-matrix(x.est[i,],nrow=npred,ncol = 2,byrow=TRUE))^2)/l^2)
}
#Calculamos la estimación
FitKernel=Kppred%*%w.hat
#Graficamos nuestra estimación
# plot3d(x[,1],x[,2],y,col=2)
# points3d(s2[,1],s2[,2],FitKernel, col = 3)
#
# #Ahora añadimos la verdadera función para evaluar la calidad de nuestra estimación
# yfun=function(x,z)5*sin(2*x)*cos(z)+sqrt(z)-x^(1/3)-0.1
# mfrow3d(1, 2, sharedMouse = TRUE)
# plot3d(x[,1],x[,2],y,col=2)
# points3d(s2[,1],s2[,2],FitKernel, col = 3)
# plot3d(yfun,xlim = c(0,glim), ylim = c(0,glim))
#~~~~~~~~~~~~~~~~~~#
# Error predictivo #
#~~~~~~~~~~~~~~~~~~#
# plot3d(x[,1],x[,2],y,col=2)
# points3d(s2[,1],s2[,2],FitKernel, col = 3)
# spheres3d(test$x[,1],test$x[,2],test$y, col = "blue", r=0.1)
#Procedemos a calcular el error cuadratico medio predictivo
ntest=dim(xnew)[1]
Kppred2=matrix(0,nrow=ntest,ncol=ntrain)
for(i in 1:ntrain)
{
Kppred2[,i]=exp(-rowSums((xnew-matrix(x.est[i,],nrow=ntest,ncol=2,byrow=TRUE))^2)/l^2)
}
ypred=Kppred2%*%w.hat
1/ntest*sum((ynew-ypred)^2)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
# Elección de los parámetros l y lambda  #
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
l.grid=seq(0.1,2,0.1)
lambda.grid=seq(0.1,2,0.1)
#En el vector aux escribimos todas las combinaciones de valores de l.grid x lambda.grid
aux=expand.grid(l.grid,lambda.grid)
#Procedemos a calcular el error cuadratico medio para todos los valores de la grilla aux. Guardamos estos valores en out.
out=rep(0,dim(aux)[1])
for(i in 1:dim(aux)[1])
{
print(paste("Iter:",i))
x.rows=split(x.est,row(x.est))
K=outer(x.rows,x.rows,Vectorize(function(x,y)Kfun(x,y,aux[i,1])))
w.hat=solve(K+aux[i,2]*diag(ntrain))%*%y
Ktest=matrix(0,nrow=ntest,ncol=ntrain)
for(j in 1:ntrain)
{
Ktest[,j]=exp(-rowSums((xnew-matrix(x.est[j,],nrow=ntest,ncol=2,byrow=TRUE))^2)/aux[i,1]^2)
}
ypred=Ktest%*%w.hat
out[i]=1/ntest*sum((ynew-ypred)^2)
}
#plot3d(aux[,1],aux[,2],out,col=2)
#Encontramos los parámetros óptimos:
indice=which(out==min(out))
aux[indice,]
View(test)
View(out)
dim(aux[1])
x.rows
dim(aux[1])
dim(aux)[1]
val = rep(0, dim_grid)
aux[indice,]
aux[indice,][0]
aux[indice,][1]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
llgrid[index,][1]
l_vec
llgrid[index,][1][1]
llgrid[index,][1][2]
llgrid[index,]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
l_vec
llgrid[index,]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
llgrid[index,]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
llgrid[index,]
llgrid[index,][1,1]
ecm_save[1]
ecm_save[25]
ecm_save[3]
ecm_save[8]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
rsq_vec2
l_vec
l_vec[1]
l_vec[2]
l_vec[2]
llgrid[index,]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
llgrid[index,]
llgrid
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
llgrid[index,]
llgrid[index,][1,1]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
rsq_vec2
l_vec[1]
l_vec[1][1]
l_vec[2]
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
l_vec[1]
l_vec[i]
l_vec[i] + lam_vec[1]
typeof(l_vec)
typeof(l_vec[1])
typeof(l_vec[1][1])
typeof(l_vec[1][2])
l_vec[1][2]
unlist(l_vec)
typeof(unlist(l_vec[1]))
unlist(l_vec) + unlist(lam_vec)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-UniversidadAdolfoIbanez/Codes/Universidad/Kernels/Repo/Tarea 2/T2Kernels.R", echo=TRUE)
