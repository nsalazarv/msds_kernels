################################################
#### Ejemplo 1: Un ejemplo de juguete en 2D ####
################################################
#*Generamos datos bastante simples para ambos grupos*
set.seed(129192)

#Tamaño de muestra para el grupo 1
n1 = 10
#Tamaño de muestra para el grupo 2
n2 = 10

#Generamos puntos del grupo 1
G1=matrix(rnorm(2*n1,mean=0,sd=1),nrow=n1,ncol=2)
#Generamos puntos del grupo 2
G2=matrix(rnorm(2*n2,mean=-6,sd=1),nrow=n2,ncol=2)

#X guarda la iformación de ambos grupos
X=rbind(G1,G2)
#En el vector Y guardamos la información de la clase a la que corresponde cada punto
Y=rep(c(1,-1),c(n1,n2))

#En indice1 guardo los datos que pertenecen al grupo 1
index1=which(Y==1)
plot(X[,1],X[,2],xlab=expression(X[1]),ylab=expression(X[2]))
points(X[index1,1],X[index1,2],pch=19,col="blue")
points(X[-index1,1],X[-index1,2],pch=19,col="magenta")

########################
## Implementación SVM ##
########################

##Función ksvm:
library("kernlab")
#Para implementar svm usaremos el metodo ksvm que recibe:

#1) y: Etiquetas con las respectivas clases de nuestros datos
#2) data: En data le pasamos nuestros puntos X en formato de matriz
#3) kernel: En este parámetro le pasamos el argumento "vanilladot" que es el algoritmo de SVM sin considerar kernels (la próxima clase implementaremos SVM usando kernels)
#4) scaled: (TRUE or FALSE), si es TRUE nuestro método escala los datos
#5) C: es la funcion de costo, C=1 por defecto.


Ksvm=ksvm(y=as.factor(Y),x=X,kernel='vanilladot',scaled=FALSE,C=1)
#En la llamada anterior es sumamente IMPORTANTE usar la función as.factor(Y), para que nuestro algoritmo entienda las clases Y como etiquetas
plot(Ksvm,data=X)


##Elementos de ksvm:
#La salida de la función ksvm entrega harta información útil. Para tener acceso a esta información usaremos el comando @ para llamar a cada parámetro. 
#Las salidas más relevantes son:
#1) @alphaindex retorna una lista con un arreglo con los indices i tal que alpha_i es "DISTINTO DE 0" en la solucion de SVM
#2) @coef retorna una lista con los coefficientes (y_i x alpha_i) distintos de 0
#3) @b retorno -w_0

indices=Ksvm@alphaindex[[1]]
coefs=Ksvm@coef[[1]]
w_0=-Ksvm@b

yalpha=rep(0,n1+n2)
yalpha[indices]=coefs
#rapidamente podemos calcular w
w = t(X)%*%yalpha

#Utilizando los parámetros w, y w0 graficamos el hiperplano que da la solución al problema de svm:
plot(X[,1],X[,2],xlab=expression(X[1]),ylab=expression(X[2]),main="Clasificación SVM")
points(X[index1,1],X[index1,2],pch=19,col="blue")
points(X[-index1,1],X[-index1,2],pch=19,col="magenta")
abline(a=-w_0/w[2],b=-w[1]/w[2],lwd=2)
abline(a=X[indices[1],]%*%w/w[2],b=-w[1]/w[2],lty=2)
abline(a=X[indices[2],]%*%w/w[2],b=-w[1]/w[2],lty=2)


##Predicción de una nueva observación:
x=matrix(c(-5,-2,-1,-2,-3,0.5),ncol=2,byrow=TRUE)

#Podemos predecir la clase de una nueva observación (o conjunto), usando la función predict
p1 = predict(Ksvm,x)

#Predecimos la nueva clase a mano
p2 = sign(x%*%w+w_0)


#Procedemos a graficar estos nuevos datos
ipred=which(p1==1)

#Utilizando los parámetros w, y w0 graficamos el hiperplano que da la solución al problema de svm:
plot(X[,1],X[,2],xlab=expression(X[1]),ylab=expression(X[2]),main="Clasificación SVM")
points(X[index1,1],X[index1,2],pch=1,col="blue")
points(X[-index1,1],X[-index1,2],pch=1,col="magenta")
abline(a=-w_0/w[2],b=-w[1]/w[2],lwd=2)
abline(a=X[indices[1],]%*%w/w[2],b=-w[1]/w[2],lty=2)
abline(a=X[indices[2],]%*%w/w[2],b=-w[1]/w[2],lty=2)
points(x[ipred,1],x[ipred,2],pch=15,col="blue")
points(x[-ipred,1],x[-ipred,2],pch=15,col="magenta")


################################################
#### Ejemplo 2: Un ejemplo de juguete en 2D ####
####            pero más difícil            ####
################################################

set.seed(129192)
#Tamaño de muestra para el grupo 1
n1 = 10
#Tamaño de muestra para el grupo 2
n2 = 10

#Generamos puntos del grupo 1
G1=matrix(rnorm(2*n1,mean=0,sd=1),nrow=n1,ncol=2)
#Generamos puntos del grupo 2
G2=matrix(rnorm(2*n2,mean=-6,sd=1),nrow=n2,ncol=2)

#X guarda la iformación de ambos grupos
X=rbind(G1,G2)
#En el vector Y guardamos la información de la clase a la que corresponde cada punto
Y=rep(c(1,-1),c(n1,n2))
Y[12]=1
Y[20]=1

#En indice1 guardo los datos que pertenecen al grupo 1
index1=which(Y==1)
plot(X[,1],X[,2],xlab=expression(X[1]),ylab=expression(X[2]))
points(X[index1,1],X[index1,2],pch=19,col="blue")
points(X[-index1,1],X[-index1,2],pch=19,col="magenta")

##~~~~~~~~~~~~~~~~~~~##
##Implementación C=0 ##
##~~~~~~~~~~~~~~~~~~~##
Ksvm=ksvm(y=as.factor(Y),x=X,kernel='vanilladot',scaled=FALSE,C=0)
##~~~~~~~~~~~~~~~~~~~~~~~~~~##
##Implementación C=0.000001 ##
##~~~~~~~~~~~~~~~~~~~~~~~~~~##
Ksvm=ksvm(y=as.factor(Y),x=X,kernel='vanilladot',scaled=FALSE,C=0.000001)
indices=Ksvm@alphaindex[[1]]
coefs=Ksvm@coef[[1]]
w_0=-Ksvm@b

yalpha=rep(0,n1+n2)
yalpha[indices]=coefs
#rapidamente podemos calcular w
w = t(X)%*%yalpha
w
#Note que los coeficientes son iguales a 0

plot(X[,1],X[,2],xlab=expression(X[1]),ylab=expression(X[2]),main="Clasificación SVM")
points(X[index1,1],X[index1,2],pch=19,col="blue")
points(X[-index1,1],X[-index1,2],pch=19,col="magenta")
abline(a=-w_0/w[2],b=-w[1]/w[2],lwd=2)

#Note que no vemos la recta que separa los puntos ya que 
#Intercepto:
-w_0/w[2]
#Pendiente
-w[1]/w[2]

##~~~~~~~~~~~~~~~~~~~##
##Implementación C=1 ##
##~~~~~~~~~~~~~~~~~~~##
Ksvm=ksvm(y=as.factor(Y),x=X,kernel='vanilladot',scaled=FALSE,C=1)
 
indices=Ksvm@alphaindex[[1]]
coefs=Ksvm@coef[[1]]
w_0=-Ksvm@b

yalpha=rep(0,n1+n2)
yalpha[indices]=coefs
#rapidamente podemos calcular w
w = t(X)%*%yalpha

#Utilizando los parámetros w, y w0 graficamos el hiperplano que da la solución al problema de svm:
plot(X[,1],X[,2],xlab=expression(X[1]),ylab=expression(X[2]),main="Clasificación SVM")
points(X[index1,1],X[index1,2],pch=19,col="blue")
points(X[-index1,1],X[-index1,2],pch=19,col="magenta")
abline(a=-w_0/w[2],b=-w[1]/w[2],lwd=2)

#o podemos usar
plot(Ksvm,data=X)

 
#para predecir usamos lo mismo que en el caso anterior
xstar = rbind(c(-1,-6),c(4,5),c(-1,-2), c(-3,2), c(-5,-4), c(0,0), c(-10,0), c(1,-1))
p1 = predict(Ksvm, xstar)
p2 = sign(xstar%*%w+w_0)


#Procedemos a graficar estos nuevos datos
ipred=which(p1==1)

#Utilizando los parámetros w, y w0 graficamos el hiperplano que da la solución al problema de svm:
plot(X[,1],X[,2],xlab=expression(X[1]),ylab=expression(X[2]),main="Clasificación SVM")
points(X[index1,1],X[index1,2],pch=1,col="blue")
points(X[-index1,1],X[-index1,2],pch=1,col="magenta")
abline(a=-w_0/w[2],b=-w[1]/w[2],lwd=2)
points(xstar[ipred,1],xstar[ipred,2],pch=15,col="blue")
points(xstar[-ipred,1],xstar[-ipred,2],pch=15,col="magenta")



#############################################
#### Ejemplo 3: Un ejemplo de Kernel SVM ####
#############################################
set.seed(129192)

##Tomemos el clasico ejemplo
n1= 10
n2=45

X1=runif(n1,0,2*pi)
X2=runif(n2,0,2*pi)
r1=1
Y1=r1*sin(X1)+rnorm(n1,0,1/4)
Z1=r1*cos(X1)+rnorm(n1,0,1/4)
r2=3
Y2=r2*sin(X2)+rnorm(n2,0,1/10)
Z2=r2*cos(X2)+rnorm(n2,0,1/10)

#Generacion de datos
Y=rep(c(1,-1),c(n1,n2))
X=cbind(c(Y1,Y2),c(Z1,Z2))
Data = as.data.frame(X)
Data$y = as.factor(Y)
#fix(Data)
names(Data) = c("x1", "x2", "y")

##El clásico


index1=which(Y==1)
plot(X[-index1,1],X[-index1,2],col='magenta',pch=19,xlab='X1',ylab='X2')
points(X[index1,1],X[index1,2],col='blue',pch=15)


########################
## Implementación SVM ##
########################

#~~~~~~~~~~~~~~~~~~#
# 1) Sin kernels   #
#~~~~~~~~~~~~~~~~~~#

#Primero tratamos de resolver este problema sin usar el kernel trick:

#Implementamos los resulatos con C=1
Ksvm = ksvm(y=as.factor(Y),x=X,kernel='vanilladot',scaled=FALSE,C=1) 
plot(Ksvm, data = X)

#Implementamos los resulatos con C=1000000
Ksvm = ksvm(y=as.factor(Y),x=X,kernel='vanilladot',scaled=FALSE,C=1000000)
plot(Ksvm, data = X)
#en los dibujos anterior note que recien con un parámetro gigante del costo llegamos algo un poco más util

#~~~~~~~~~~~~~~~~~~~~~~~~~#
# 2) Utilizando kernels   #
#~~~~~~~~~~~~~~~~~~~~~~~~~#

#La funcion Ksvm trae algunos kernels por defecto. Por ejemplo: rbfdot, y polydot. Vemos como se comporta KSVM con el kernel rbfdot con distintos parametros

Ksvm=ksvm(y=as.factor(Y),x=X,kernel='rbfdot',kpar=list(sigma=0.1),scaled=FALSE,C=1)
plot(Ksvm, data = X)

Ksvm=ksvm(y=as.factor(Y),x=X,kernel='rbfdot',kpar=list(sigma=1),scaled=FALSE,C=1) 
plot(Ksvm, data = X)

Ksvm=ksvm(y=as.factor(Y),x=X,kernel='rbfdot',kpar=list(sigma=10),scaled=FALSE,C=1) 
plot(Ksvm, data = X)


##Prediccion
x=matrix(c(-1.2,-0.5,-1,-2,-0.2,0.5),ncol=2,byrow=TRUE)

#Podemos predecir la clase de una nueva observación (o conjunto), usando la función predict
p1=predict(Ksvm,x)

plot(Ksvm,data=X)
points(x[,1],x[,2],col='green',pch=17,lwd=3)

#~~~~~~~~~~~~~~~~~~~~~~~~~#
# 3) Utilizando kernels   #
#~~~~~~~~~~~~~~~~~~~~~~~~~#
#Si queremos pasarle una matriz de kernel a mano podemos hacer lo siguiente
Kfun=function(x,y,sigma)
{
  exp(-sigma*sum((x-y)^2))
}

#Calculamos el kernel
n=dim(X)[1]
K = matrix(0,nrow=n,ncol=n)

for(i in 1:n)
{
  for(j in 1:n)
  {
    K[i,j]=Kfun(X[i,],X[j,],1)
  }
}

#En x le pasamos la matriz de kernel, y en kernel le indicamos que le estamos pasando una matriz con el comando 'matrix'
Ksvm=ksvm(y=as.factor(Y),x=K,kernel='matrix',scaled=FALSE,C=1)

